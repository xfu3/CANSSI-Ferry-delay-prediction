{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1668,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib import dates\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import SGDClassifier, LassoCV\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.ensemble import VotingClassifier \n",
    "import warnings\n",
    "import holidays\n",
    "import calendar\n",
    "from varname import nameof\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function of getting the optimized paramters and score\n",
    "def hypertuning_rscv(est,p_distr,nbr_iter,X,y):\n",
    "    rdmsearch = RandomizedSearchCV(est, param_distributions=p_distr,n_jobs=-1, n_iter=nbr_iter,cv=5)\n",
    "    \n",
    "    rdmsearch.fit(X,y)\n",
    "    ht_params = rdmsearch.best_params_\n",
    "    ht_score = rdmsearch.best_score_\n",
    "    return(ht_params,ht_score)\n",
    "\n",
    "## submission\n",
    "def submission(clf,test,classifier):\n",
    "    preds = clf.predict_proba(test)\n",
    "    test_new = test.copy()\n",
    "    test_new[\"ID\"] = test_new.index +1\n",
    "    ctbsubmission = pd.concat([pd.DataFrame(preds[:,1]),test_new['ID']],axis=1)\n",
    "    ctbsubmission.columns = ['Delay.Indicator','ID']\n",
    "    ctbsubmission.to_csv('{}_predictions_1.csv'.format(classifier),index=False)\n",
    "    \n",
    "def add_num_sailings_ratio(data):\n",
    "    avgs = data.groupby(['Year','Month'], as_index=False)['Trip'].count()\n",
    "\n",
    "    avgs['Trip'] = avgs['Trip']/data.groupby(['Year','Month'], as_index=False)['Day.of.Month'].nunique()\n",
    "    tripCounts = pd.merge(data, avgs.rename(columns={'Trip':'avg.Trips'}), how='left', on=['Year', 'Month'])\n",
    "    tripCounts_with_date = pd.merge(tripCounts, data.groupby('Full.Date', as_index=False)['Trip'].count().rename(columns={'Trip':'num.Trips'}), how='left', on='Full.Date')\n",
    "    \n",
    "    tripCounts_with_date['Sailings.Ratio'] = tripCounts_with_date['num.Trips']/tripCounts_with_date['avg.Trips']\n",
    "    return tripCounts_with_date.drop(columns=['avg.Trips', 'num.Trips'])\n",
    "\n",
    "def add_weather_events(dates):\n",
    "    weather_events = severe_weather_dates = ['04 December 2016','05 December 2016','24 November 2016', \n",
    "                                             '14 October 2016', '13 November 2017', '20 December 2018', '19 November 2017', \n",
    "                                             '14 November 2017', '15 November 2017', '07 April 2017', '21 January 2018', '21 January 2018', '28 February 2018']\n",
    "    severe_weather = dates.isin(weather_events)\n",
    "    return severe_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1670,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOptimizer:\n",
    "    best_score = None\n",
    "    opt = None\n",
    "    \n",
    "    def __init__(self, model, X_train, y_train, categorical_columns_indices=None, n_fold=5, seed=2405, early_stopping_rounds=30, is_stratified=True, is_shuffle=True):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.categorical_columns_indices = categorical_columns_indices\n",
    "        self.n_fold = n_fold\n",
    "        self.seed = seed\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.is_stratified = is_stratified\n",
    "        self.is_shuffle = is_shuffle\n",
    "        \n",
    "        \n",
    "    def update_model(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self.model, k, v)\n",
    "            \n",
    "    def evaluate_model(self):\n",
    "        pass\n",
    "    \n",
    "    def optimize(self, param_space, max_evals=10, n_random_starts=2): ## Baysian \n",
    "        start_time = time.time()\n",
    "        \n",
    "        @use_named_args(param_space)\n",
    "        def _minimize(**params):\n",
    "            self.model.set_params(**params)\n",
    "            return self.evaluate_model()\n",
    "        \n",
    "        opt = gp_minimize(_minimize, param_space, n_calls=max_evals, n_random_starts=n_random_starts, random_state=2405, n_jobs=-1)\n",
    "        best_values = opt.x\n",
    "        optimal_values = dict(zip([param.name for param in param_space], best_values))\n",
    "        best_score = opt.fun\n",
    "        self.best_score = best_score\n",
    "        self.opt = opt\n",
    "        \n",
    "        print('optimal_parameters: {}\\noptimal score: {}\\noptimization time: {}'.format(optimal_values, best_score, time.time() - start_time))\n",
    "        print('updating model with optimal values')\n",
    "        self.update_model(**optimal_values)\n",
    "        plot_convergence(opt)\n",
    "        return optimal_values\n",
    "\n",
    "class XgbOptimizer(ModelOptimizer):\n",
    "    def evaluate_model(self):\n",
    "        scores = xgboost.cv(self.model.get_xgb_params(), \n",
    "                    xgboost.DMatrix(self.X_train, label=self.y_train),\n",
    "                    num_boost_round=self.model.n_estimators, \n",
    "                    metrics='auc', \n",
    "                    nfold=self.n_fold, \n",
    "                    stratified=self.is_stratified,\n",
    "                    shuffle=self.is_shuffle,\n",
    "                    seed=self.seed,\n",
    "                    early_stopping_rounds=self.early_stopping_rounds)\n",
    "        self.scores = scores\n",
    "        test_scores = scores.iloc[:, 2]\n",
    "        best_metric = test_scores.max()\n",
    "        return 1 - best_metric\n",
    "    \n",
    "class CatboostOptimizer(ModelOptimizer):\n",
    "    def evaluate_model(self):\n",
    "        validation_scores = catboost.cv(\n",
    "        catboost.Pool(self.X_train, \n",
    "                      self.y_train, \n",
    "                      cat_features=self.categorical_columns_indices),\n",
    "        self.model.get_params(), \n",
    "        nfold=self.n_fold,\n",
    "        stratified=self.is_stratified,\n",
    "        seed=self.seed,\n",
    "        early_stopping_rounds=self.early_stopping_rounds,\n",
    "        shuffle=self.is_shuffle,\n",
    "        plot=False)\n",
    "        self.scores = validation_scores\n",
    "        test_scores = validation_scores.iloc[:, 2]\n",
    "        best_metric = test_scores.max()\n",
    "        return 1 - best_metric\n",
    "    \n",
    "class LightGBMOptimizer(ModelOptimizer):\n",
    "    def evaluate_model(self):\n",
    "        lgb_dataset = lgb.Dataset(self.X_train, \n",
    "                                  self.y_train, \n",
    "                                  self.categorical_columns_indices)\n",
    "        eval_hist = lgb.cv(self.model.get_params(), \n",
    "                           lgb_dataset,\n",
    "                           self.model.n_estimators, \n",
    "                           nfold=self.n_fold,\n",
    "                           seed=self.seed, \n",
    "                           stratified=self.is_stratified, \n",
    "                           shuffle=self.is_shuffle,\n",
    "                           early_stopping_rounds=self.early_stopping_rounds, \n",
    "                           metrics='auc')\n",
    "        self.scores = eval_hist\n",
    "        test_scores = eval_hist[list(eval_hist.keys())[0]]\n",
    "        best_metric = max(test_scores)\n",
    "        return 1 - best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"traffic_clean3.csv\")\n",
    "test = pd.read_csv(\"test_clean3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary feature preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the Vessel.name categories that are not present in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have noticed that there are some Vessel.Name categorical levels in the training dataset that are not present in the test data. Considering that the redundant information would negatively influence the predictive power of our models, we decided to cut out the Vessel name categorical levels that are not seen in the test data from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1714,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_redun = [i for i in np.unique(train[\"Vessel.Name\"]) if i not in np.unique(test[\"Vessel.Name\"])]\n",
    "train = train.loc[~train[\"Vessel.Name\"].isin(vessel_redun),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize the traffic lags from 0 to 5 to reduce noises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distribution of the lag variables are significant imbalanced, we decided to categorize traffic lags into categroical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [],
   "source": [
    "trai = train1[\"Scheduled.Departure\"].apply(lambda x: pd.to_datetime(x).strftime(\"%H:%M\"))\n",
    "tes = test[\"Scheduled.Departure\"].apply(lambda x: pd.to_datetime(x).strftime(\"%H:%M\"))\n",
    "vessel_sche = [i for i in np.unique(trai) if i not in np.unique(tes)]\n",
    "train = train.loc[~train[\"Scheduled.Departure\"].isin(vessel_sche),:]\n",
    "\n",
    "train1.reset_index(drop = True,inplace = True)\n",
    "\n",
    "## train cut\n",
    "train[\"cut1\"] = pd.cut(train.lag1 , [0,2,3,4,5], right=True)\n",
    "train[\"cut2\"] = pd.cut(train.lag2 , [0,2,3,4,5], right=True)\n",
    "train[\"cut3\"] = pd.cut(train.lag3 , [0,2,3,4,5], right=True)\n",
    "train[\"cut4\"] = pd.cut(train.lag4 , [0,2,3,4,5], right=True)\n",
    "\n",
    "## test cut\n",
    "test[\"cut1\"] = pd.cut(test.lag1 , [0,2,3,4,5], right=True)\n",
    "test[\"cut2\"] = pd.cut(test.lag2 , [0,2,3,4,5], right=True)\n",
    "test[\"cut3\"] = pd.cut(test.lag3 , [0,2,3,4,5], right=True)\n",
    "test[\"cut4\"] = pd.cut(test.lag4 , [0,2,3,4,5], right=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Vessel categories with similar delay frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the delay frequency by category\n",
    "train[[\"Vessel.Name\",\"Delay.Indicator\"]].groupby(\"Vessel.Name\").mean().sort_values(by = 'Delay.Indicator')\n",
    "\n",
    "## combine the Vessel categories with similar delay frequencies\n",
    "train.loc[train[\"Vessel.Name\"].isin([\"Skeena Queen\",\"Mayne Queen\",'Queen of Alberni']),\"Vessel\"] = \"name1\"\n",
    "train.loc[train[\"Vessel.Name\"].isin([\"Coastal Renaissance\",\"Queen of New Westminster\",\"Bowen Queen\",\"Coastal Inspiration\",\"Queen of Cumberland\",\"Coastal Celebration\"]),\"Vessel\"] = \"name2\"\n",
    "train.loc[train[\"Vessel.Name\"].isin(['Queen of Coquitlam',\"Spirit of Vancouver Island\",\"Queen of Cowichan\",\"Queen of Capilano\"]),\"Vessel\"] = \"name3\"\n",
    "train.loc[train[\"Vessel.Name\"].isin([\"Queen of Oak Bay\",\"Salish Raven\",\"Queen of Surrey\",\"Salish Eagle\"]),\"Vessel\"] = \"name4\"\n",
    "\n",
    "test.loc[train[\"Vessel.Name\"].isin([\"Skeena Queen\",\"Mayne Queen\",'Queen of Alberni']),\"Vessel\"] = \"name1\"\n",
    "test.loc[train[\"Vessel.Name\"].isin([\"Coastal Renaissance\",\"Queen of New Westminster\",\"Bowen Queen\",\"Coastal Inspiration\",\"Queen of Cumberland\",\"Coastal Celebration\"]),\"Vessel\"] = \"name2\"\n",
    "test.loc[train[\"Vessel.Name\"].isin(['Queen of Coquitlam',\"Spirit of Vancouver Island\",\"Queen of Cowichan\",\"Queen of Capilano\"]),\"Vessel\"] = \"name3\"\n",
    "test.loc[train[\"Vessel.Name\"].isin([\"Queen of Oak Bay\",\"Salish Raven\",\"Queen of Surrey\",\"Salish Eagle\"]),\"Vessel\"] = \"name4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the feature Round Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1675,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the feature Round Hour\n",
    "train[\"Date_time\"] = pd.to_datetime(train[\"Date_time\"])\n",
    "test[\"Date_time\"] = pd.to_datetime(test[\"Date_time\"])\n",
    "\n",
    "train[\"Round_time\"] = train[\"Date_time\"].dt.round(\"H\")\n",
    "train[\"Round_Hour\"] = train[\"Round_time\"].apply(lambda x: x.strftime(\"%H\"))\n",
    "\n",
    "test[\"Round_time\"] = test[\"Date_time\"].dt.round(\"H\")\n",
    "test[\"Round_Hour\"] = test[\"Round_time\"].apply(lambda x: x.strftime(\"%H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1676,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test.Round_Hour == \"00\",\"Round_Hour\"] = \"23\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1677,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Month\"] = train[\"Month\"].astype(\"object\")\n",
    "train[\"Day.of.Month\"] = train[\"Day.of.Month\"].astype('int')\n",
    "train.loc[train[\"Round_Hour\"].isin([\"23\",\"00\"]),\"Round_Hour\"] = \"23\"\n",
    "\n",
    "test[\"Month\"] = test[\"Month\"].astype(\"object\")\n",
    "test[\"Day.of.Month\"] = test[\"Day.of.Month\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1678,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train1.loc[~train1[\"Status\"].isin([\"Mechanical issue\",\"Unknown\"]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1727,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train1 = train1.loc[train1[\"Month\"].isin([1,2,3,11,12]),:]\n",
    "#train1 = train1.loc[~train1[\"Status\"].isin([\"Mechanical issue\"]),:]\n",
    "X2 = train[['Round_Hour','Day',\"Sailings.Ratio\",'Severe.Weather','Scheduled.Departure.min',\n",
    "       'Vessel.Name',\"Trip_Kfold_Target_Enc\",\"holidays_indicator\",'lag2','lag3','lag4','lag_std2',\"Summer.Season\",'off.Season']]\n",
    "\n",
    "\n",
    "X2_enc = pd.get_dummies(X2,prefix=['Round_Hour','Vessel.Name','Day'])\n",
    "\n",
    "y = train[\"Delay.Indicator\"]\n",
    "\n",
    "X_test = test[['Round_Hour','Day',\"Sailings.Ratio\",'Severe.Weather','Scheduled.Departure.min',\n",
    "       'Vessel.Name',\"Trip_Kfold_Target_Enc\",\"holidays_indicator\",'lag2','lag3','lag4','lag_std2',\"Summer.Season\",'off.Season']]\n",
    "\n",
    "X2_test_enc = pd.get_dummies(X_test,prefix=['Round_Hour','Vessel.Name','Day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression[**undersampling**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1728,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample_pipe = make_pipeline(RandomUnderSampler(), LogisticRegression())\n",
    "\n",
    "param_grid = {'logisticregression__penalty':[\"l2\"],\n",
    "              'logisticregression__C': [1,5,10,15]}\n",
    "nbr_iter=200\n",
    "random_params, random_score = hypertuning_rscv(undersample_pipe, param_grid,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__penalty': 'l2', 'logisticregression__C': 15}"
      ]
     },
     "execution_count": 1729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1730,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=36, shuffle=True)\n",
    "undersample = make_pipeline(RandomUnderSampler(), LogisticRegression(penalty = 'l2',C = 15))\n",
    "\n",
    "cv_results_auc_log = cross_val_score(undersample, X2_enc, y, scoring='roc_auc', cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745689611257359"
      ]
     },
     "execution_count": 1731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_auc_log.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('randomundersampler', RandomUnderSampler()),\n",
       "                ('logisticregression', LogisticRegression(C=15))])"
      ]
     },
     "execution_count": 1732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersample.fit(X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1733,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logistic regression prediction submitted\n",
    "submission(undersample,X2_test_enc,\"undersample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression[**oversampling**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1734,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample_pipe = make_pipeline(SMOTE(random_state=42), LogisticRegression())\n",
    "\n",
    "param_grid = {'logisticregression__penalty':[\"l2\"],\n",
    "              'logisticregression__C': [1,5,10,15]}\n",
    "nbr_iter=200\n",
    "random_params, random_score = hypertuning_rscv(oversample_pipe, param_grid,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__penalty': 'l2', 'logisticregression__C': 15}"
      ]
     },
     "execution_count": 1735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1736,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "oversample = make_pipeline(SMOTE(random_state=42), LogisticRegression(penalty = 'l2',C = 15))\n",
    "\n",
    "cv_results_auc_log = cross_val_score(oversample, X2_enc, y, scoring='roc_auc', cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7398590218553251"
      ]
     },
     "execution_count": 1737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_auc_log.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('smote', SMOTE(random_state=42)),\n",
       "                ('logisticregression', LogisticRegression(C=15))])"
      ]
     },
     "execution_count": 1738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample.fit(X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1739,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logistic regression prediction submitted\n",
    "submission(oversample,X2_test_enc,\"oversample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression[**balanced**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1740,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'penalty':[\"l2\"],\n",
    "              'C': [1,5,10,15,20],\n",
    "              'class_weight':[\"balanced\"]}\n",
    "nbr_iter=200\n",
    "random_params, random_score = hypertuning_rscv(LogisticRegression(), param_grid,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2', 'class_weight': 'balanced', 'C': 5}"
      ]
     },
     "execution_count": 1741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1744,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_log = LogisticRegression(penalty = 'l2',C = 5,class_weight ='balanced')\n",
    "\n",
    "cv_results_auc_log = cross_val_score(balance_log, X2_enc, y, scoring='roc_auc', cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7463752220717775"
      ]
     },
     "execution_count": 1745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_auc_log.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1746,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, class_weight='balanced')"
      ]
     },
     "execution_count": 1746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_log.fit(X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1747,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logistic regression prediction submitted\n",
    "submission(balance_log,X2_test_enc,\"balance_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For random forest implementation, we use random search to tune hyperparameters to find the maximal 3 fold cross validation\n",
    "- Oversampling (SMOTE)\n",
    "- Undersampling \n",
    "- class_weight (a hyperparameter accomodating imbalanced dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling by SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion':['gini'],\n",
    "    'min_samples_split':[2,3,5,6,8,10,12],\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [i for i in range(1,6)],\n",
    "    'max_features': [i for i in range(6,12)],\n",
    "    'min_samples_leaf': [3,4,5,6,7],\n",
    "    'min_samples_split': [2,3,5,6],\n",
    "    'n_estimators': [300,500,800],\n",
    "    'ccp_alpha':[1,5,10,20]\n",
    "}\n",
    "nbr_iter = 200\n",
    "new_params = {'randomforestclassifier__' + key: param_grid[key] for key in param_grid}\n",
    "imba_pipeline = make_pipeline(SMOTE(random_state=42),RandomForestClassifier(random_state=13))\n",
    "\n",
    "\n",
    "random_params, random_score = hypertuning_rscv(imba_pipeline, new_params,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 500,\n",
       " 'randomforestclassifier__min_samples_split': 5,\n",
       " 'randomforestclassifier__min_samples_leaf': 6,\n",
       " 'randomforestclassifier__max_features': 8,\n",
       " 'randomforestclassifier__max_depth': 5,\n",
       " 'randomforestclassifier__criterion': 'gini',\n",
       " 'randomforestclassifier__bootstrap': True}"
      ]
     },
     "execution_count": 1209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1612,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 500,\n",
    " min_samples_split = 5,\n",
    " min_samples_leaf = 6,\n",
    " max_features = 8,\n",
    " max_depth = 5,\n",
    " criterion = 'gini',\n",
    " bootstrap = True,\n",
    " n_jobs=-1)\n",
    "\n",
    "imba_pipeline_rf_SMOTE = make_pipeline(SMOTE(random_state=42),clf)\n",
    "\n",
    "crossval_scores = cross_validate(imba_pipeline_rf_SMOTE, X2_enc, y, cv = 10,scoring = \"roc_auc\",return_train_score =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7246595058875503"
      ]
     },
     "execution_count": 1613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(crossval_scores[\"test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest prediction submmited (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X2_enc,y)\n",
    "submission(imba_pipeline_rf_SMOTE,X2_test_enc,\"forest_over_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resampling\n",
    "resample = RandomUnderSampler()\n",
    "\n",
    "## undersampling pipeline\n",
    "imba_pipeline_rf_un = make_pipeline(resample,RandomForestClassifier(random_state=13))\n",
    "\n",
    "## tuned hyperparameter\n",
    "random_params, random_score = hypertuning_rscv(imba_pipeline_rf_un, new_params,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 500,\n",
       " 'randomforestclassifier__min_samples_split': 5,\n",
       " 'randomforestclassifier__min_samples_leaf': 6,\n",
       " 'randomforestclassifier__max_features': 8,\n",
       " 'randomforestclassifier__max_depth': 5,\n",
       " 'randomforestclassifier__criterion': 'gini',\n",
       " 'randomforestclassifier__bootstrap': True}"
      ]
     },
     "execution_count": 1210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "resample = RandomUnderSampler()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 500,\n",
    " min_samples_split = 5,\n",
    " min_samples_leaf = 7,\n",
    " max_features = 6,\n",
    " max_depth = 5,\n",
    " criterion = 'gini',\n",
    " bootstrap = True,\n",
    " n_jobs=-1)\n",
    "\n",
    "imba_pipeline_rf = make_pipeline(resample,clf)\n",
    "\n",
    "## cross validation\n",
    "crossval_scores = cross_validate(imba_pipeline_rf, X2_enc, y, cv = 10,scoring = \"roc_auc\",return_train_score =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6723668944829448"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(crossval_scores['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split':[2,3,5,6,8,10,12],\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [i for i in range(1,6)],\n",
    "    'max_features': [i for i in range(6,12)],\n",
    "    'min_samples_leaf': [3,4,5,6,7],\n",
    "    'min_samples_split': [2,3,5,6],\n",
    "    'n_estimators': [300,500,800],\n",
    "    \"class_weight\":[\"balanced\",\"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "clf_3 = RandomForestClassifier()\n",
    "\n",
    "random_params, random_score = hypertuning_rscv(clf_3, param_grid,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__penalty': 'l2', 'logisticregression__C': 5}"
      ]
     },
     "execution_count": 1461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1469,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300,\n",
    " min_samples_split = 3,\n",
    " min_samples_leaf = 4,\n",
    " max_features = 10,\n",
    " max_depth = 1,\n",
    " criterion = 'entropy',\n",
    " class_weight = 'balanced_subsample',\n",
    " bootstrap = True,\n",
    " n_jobs=-1)\n",
    "\n",
    "## cross validation\n",
    "crossval_scores = cross_validate(clf, X2_enc, y, cv = 10,scoring = \"roc_auc\",return_train_score =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6929074724338746"
      ]
     },
     "execution_count": 1470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X2_enc,y)\n",
    "\n",
    "submission(clf,X2_test_enc,\"forest_under_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Round_Hour</th>\n",
       "      <th>Day</th>\n",
       "      <th>Scheduled.Departure</th>\n",
       "      <th>Vessel.Name</th>\n",
       "      <th>Trip_Kfold_Target_Enc</th>\n",
       "      <th>holidays_indicator</th>\n",
       "      <th>weekend.Indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05</td>\n",
       "      <td>6</td>\n",
       "      <td>315</td>\n",
       "      <td>Queen of Alberni</td>\n",
       "      <td>0.112426</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05</td>\n",
       "      <td>6</td>\n",
       "      <td>315</td>\n",
       "      <td>Coastal Inspiration</td>\n",
       "      <td>0.126398</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06</td>\n",
       "      <td>6</td>\n",
       "      <td>330</td>\n",
       "      <td>Queen of Cumberland</td>\n",
       "      <td>0.116782</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06</td>\n",
       "      <td>6</td>\n",
       "      <td>360</td>\n",
       "      <td>Mayne Queen</td>\n",
       "      <td>0.116782</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06</td>\n",
       "      <td>6</td>\n",
       "      <td>380</td>\n",
       "      <td>Coastal Renaissance</td>\n",
       "      <td>0.232172</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45781</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1260</td>\n",
       "      <td>Spirit of Vancouver Island</td>\n",
       "      <td>0.168478</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45782</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1260</td>\n",
       "      <td>Skeena Queen</td>\n",
       "      <td>0.038743</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45783</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1260</td>\n",
       "      <td>Coastal Celebration</td>\n",
       "      <td>0.169185</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45784</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1350</td>\n",
       "      <td>Queen of Cowichan</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45785</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1365</td>\n",
       "      <td>Queen of Coquitlam</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45786 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Round_Hour Day  Scheduled.Departure                 Vessel.Name  \\\n",
       "0             05   6                  315            Queen of Alberni   \n",
       "1             05   6                  315         Coastal Inspiration   \n",
       "2             06   6                  330         Queen of Cumberland   \n",
       "3             06   6                  360                 Mayne Queen   \n",
       "4             06   6                  380         Coastal Renaissance   \n",
       "...          ...  ..                  ...                         ...   \n",
       "45781         21   0                 1260  Spirit of Vancouver Island   \n",
       "45782         21   0                 1260                Skeena Queen   \n",
       "45783         21   0                 1260         Coastal Celebration   \n",
       "45784         22   0                 1350           Queen of Cowichan   \n",
       "45785         23   0                 1365          Queen of Coquitlam   \n",
       "\n",
       "       Trip_Kfold_Target_Enc  holidays_indicator  weekend.Indicator  \n",
       "0                   0.112426               False               True  \n",
       "1                   0.126398               False               True  \n",
       "2                   0.116782               False               True  \n",
       "3                   0.116782               False               True  \n",
       "4                   0.232172               False               True  \n",
       "...                      ...                 ...                ...  \n",
       "45781               0.168478               False              False  \n",
       "45782               0.038743               False              False  \n",
       "45783               0.169185               False              False  \n",
       "45784               0.100438               False              False  \n",
       "45785               0.100438               False              False  \n",
       "\n",
       "[45786 rows x 7 columns]"
      ]
     },
     "execution_count": 1068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1462,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2[\"Vessel.Name\"] = X2[\"Vessel.Name\"].astype(\"category\")\n",
    "X2[\"Round_Hour\"] = X2[\"Round_Hour\"].astype(\"category\")\n",
    "X2[\"Day\"] = X2[\"Day\"].astype(\"category\")\n",
    "#X2[\"Day.of.Month\"] = X2[\"Day.of.Month\"].astype(\"category\")\n",
    "#X2[\"Month\"] = X2[\"Month\"].astype(\"category\")\n",
    "\n",
    "X_test2[\"Vessel.Name\"] = X_test2[\"Vessel.Name\"].astype(\"category\")\n",
    "X_test2[\"Round_Hour\"] = X_test2[\"Round_Hour\"].astype(\"category\")\n",
    "X_test2[\"Day\"] = X_test2[\"Day\"].astype(\"category\")\n",
    "#X_test2[\"Day.of.Month\"] = X_test2[\"Day.of.Month\"].astype(\"category\")\n",
    "#X_test2[\"Month\"] = X_test2[\"Month\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X2[\"Month\"] = X2[\"Month\"].astype(\"category\")\n",
    "#X2[\"Day.of.Month\"] = X2[\"Day.of.Month\"].astype(\"category\")\n",
    "X2_encode = pd.get_dummies(X2,prefix=[\"Round_Hour\",\"Day\",\"Vessel.Name\"])\n",
    "\n",
    "#X_test2[\"Day.of.Month\"] = X_test2[\"Day.of.Month\"].astype(\"category\")\n",
    "#X_test2[\"Month\"] = X_test2[\"Month\"].astype(\"category\")\n",
    "X_test_encode = pd.get_dummies(X_test2,prefix=[\"Round_Hour\",\"Day\",\"Vessel.Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_encode.rename(columns = {\"cut4_(0, 2]\":\"cut1\",\"cut4_(2, 3]\":\"cut2\",\"cut4_(3, 4]\":\"cut3\",\"cut4_(4, 5]\":\"cut4\"},inplace =True)\n",
    "X_test_encode.rename(columns = {\"cut4_(0, 2]\":\"cut1\",\"cut4_(2, 3]\":\"cut2\",\"cut4_(3, 4]\":\"cut3\",\"cut4_(4, 5]\":\"cut4\"},inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X2_encode, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "p_distr ={\n",
    "        \"objective\":['reg:logistic'],\n",
    "        \"booster\":[\"gbtree\"],\n",
    "        'n_estimators': [700],\n",
    "        'min_child_weight': list(range(1,10,3)),\n",
    "        'gamma': [3,5],\n",
    "        'subsample': [0.6,0.7],\n",
    "        'colsample_bytree': [0.7],\n",
    "        'max_depth': list(range(8,12,3)),\n",
    "        'eta': [0.1],  \n",
    "        \"scale_pos_weight\":[3,5]\n",
    "}\n",
    "model = XGBClassifier()\n",
    "nbr_iter = 168\n",
    "random_params, random_score = hypertuning_rscv(model, p_distr, nbr_iter,X2_encode,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1465,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_params ={'subsample': 0.7,\n",
    " 'scale_pos_weight': 3,\n",
    " 'objective': 'reg:logistic',\n",
    " 'n_estimators': 700,\n",
    " 'min_child_weight': 1,\n",
    " 'max_depth': 11,\n",
    " 'gamma': 3,\n",
    " 'eta': 0.1,\n",
    " 'colsample_bytree': 0.7,\n",
    " 'booster': 'gbtree'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:57:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " cv_results = xgb.cv(dtrain= dtrain, params=random_params, nfold=10,num_boost_round=300,\n",
    "                     metrics='auc', early_stopping_rounds = 50,\n",
    "                        as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.812238</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.759261</td>\n",
       "      <td>0.017622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.868255</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>0.811136</td>\n",
       "      <td>0.022856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.886524</td>\n",
       "      <td>0.008649</td>\n",
       "      <td>0.823995</td>\n",
       "      <td>0.020480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.901068</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.835277</td>\n",
       "      <td>0.018635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.910832</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.842575</td>\n",
       "      <td>0.016510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.986125</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.884583</td>\n",
       "      <td>0.017071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.986250</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.884650</td>\n",
       "      <td>0.017052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.986334</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.884631</td>\n",
       "      <td>0.016979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.986453</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.884686</td>\n",
       "      <td>0.016916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.986548</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.884691</td>\n",
       "      <td>0.017078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0          0.812238       0.005044       0.759261      0.017622\n",
       "1          0.868255       0.008382       0.811136      0.022856\n",
       "2          0.886524       0.008649       0.823995      0.020480\n",
       "3          0.901068       0.006645       0.835277      0.018635\n",
       "4          0.910832       0.003532       0.842575      0.016510\n",
       "..              ...            ...            ...           ...\n",
       "119        0.986125       0.000433       0.884583      0.017071\n",
       "120        0.986250       0.000466       0.884650      0.017052\n",
       "121        0.986334       0.000495       0.884631      0.016979\n",
       "122        0.986453       0.000492       0.884686      0.016916\n",
       "123        0.986548       0.000536       0.884691      0.017078\n",
       "\n",
       "[124 rows x 4 columns]"
      ]
     },
     "execution_count": 1467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.7,\n",
       " 'scale_pos_weight': 3,\n",
       " 'objective': 'reg:logistic',\n",
       " 'n_estimators': 700,\n",
       " 'min_child_weight': 1,\n",
       " 'max_depth': 11,\n",
       " 'gamma': 3,\n",
       " 'eta': 0.1,\n",
       " 'colsample_bytree': 0.7,\n",
       " 'booster': 'gbtree'}"
      ]
     },
     "execution_count": 1048,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:24:18] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { num_boost_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBClassifier(subsample = 0.7,\n",
    "              scale_pos_weight = 3,\n",
    "              objective = \"reg:logistic\",\n",
    "              min_child_weight = 1,\n",
    "              n_estimators = 700,\n",
    "              max_depth = 8,\n",
    "              gamma = 3,\n",
    "              eta = 0.1,\n",
    "              colsample_bytree = 0.7,\n",
    "              booster = 'gbtree',\n",
    "              num_boost_round=300)\n",
    "\n",
    "xg_reg.fit(X2_enc,y)\n",
    "## submission\n",
    "submission(xg_reg,X2_test_enc,\"xg_reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM hyperparameter tuning<a class=\"tocSkip\">\n",
    "Since hyperparameter tuning is a time consuming process, we will take 7 steps to perform hyperparamter tuning by AUC: \n",
    "- step 1: tuning **max_depth** and **num_leaves** (tree complexity)\n",
    "- step 2: tuning **min_data_in_leaf** and **min_sum_hessian_in_leaf** (prevent overfitting)\n",
    "- step 3: tuning **feature_fraction** (prevent overfitting, decorrelate trees)\n",
    "- step 4: tuning **bagging_fraction** and **bagging_freq** (prevent overfitting)\n",
    "- step 5: tuning **lambda_l1(reg_alpha)** and **lambda_l2(reg_lambda)** (prevent overfitting)\n",
    "- step 6: tuning **cat_smooth** (reduce the effect of noises in categorical features)\n",
    "- step 7: tuning **learning_rate** and **num_iterations** (final tuning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X2_encode,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning all lightGBM hyperparameters in one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_lgb = {\n",
    "    'max_depth': [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],\n",
    "    'num_leaves': [40,50,60,70,80,90,100],\n",
    "    'min_data_in_leaf':range(1,102,10),\n",
    "    'min_sum_hessian_in_leaf':[0.02,0.03,0.04,0.05],\n",
    "    'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "    'bagging_freq': [2, 4, 5, 6, 8],\n",
    "    'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n",
    "    'lambda_l2': [0, 10, 15, 35, 40],\n",
    "    'cat_smooth': [1, 10, 15, 20, 35],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "    #'num_iterations':range(100,1000,100)\n",
    "}\n",
    "\n",
    "\n",
    "model_lgb = lgb.LGBMClassifier(is_unbalance = True,metric = 'auc')\n",
    "\n",
    "random_params_lgb, random_score_lgb = hypertuning_rscv(model_lgb, para_lgb,nbr_iter,X2_encode,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 40,\n",
       " 'min_sum_hessian_in_leaf': 0.04,\n",
       " 'min_data_in_leaf': 61,\n",
       " 'max_depth': 4,\n",
       " 'learning_rate': 0.01,\n",
       " 'lambda_l2': 35,\n",
       " 'lambda_l1': 0,\n",
       " 'feature_fraction': 0.6,\n",
       " 'cat_smooth': 20,\n",
       " 'bagging_freq': 8,\n",
       " 'bagging_fraction': 0.7}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning by min_data_in_leaf and min_sum_hessian_in_leaf  (step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's train auc: 0.775119 + 0.00111859\tcv_agg's valid auc: 0.769922 + 0.0104099\n",
      "[200]\tcv_agg's train auc: 0.787696 + 0.00127297\tcv_agg's valid auc: 0.781594 + 0.00964075\n",
      "[300]\tcv_agg's train auc: 0.796683 + 0.00114024\tcv_agg's valid auc: 0.789772 + 0.0100093\n",
      "[400]\tcv_agg's train auc: 0.803041 + 0.0012455\tcv_agg's valid auc: 0.795453 + 0.0100901\n",
      "[500]\tcv_agg's train auc: 0.80791 + 0.00112774\tcv_agg's valid auc: 0.799375 + 0.0101369\n",
      "[600]\tcv_agg's train auc: 0.811825 + 0.0010572\tcv_agg's valid auc: 0.802474 + 0.0103189\n",
      "[700]\tcv_agg's train auc: 0.815412 + 0.00111558\tcv_agg's valid auc: 0.805255 + 0.0102686\n",
      "[800]\tcv_agg's train auc: 0.81867 + 0.0010543\tcv_agg's valid auc: 0.807684 + 0.0104024\n",
      "[900]\tcv_agg's train auc: 0.822027 + 0.00098294\tcv_agg's valid auc: 0.810171 + 0.0104422\n",
      "[1000]\tcv_agg's train auc: 0.824941 + 0.00110989\tcv_agg's valid auc: 0.812159 + 0.0105018\n"
     ]
    }
   ],
   "source": [
    "# LightGBM, cross-validation\n",
    "cv_result_lgb = lgb.cv(random_params_lgb, \n",
    "                       dtrain, \n",
    "                       num_boost_round = 1000, \n",
    "                       metrics = \"auc\",\n",
    "                       nfold=10, \n",
    "                       stratified=True, \n",
    "                       early_stopping_rounds=50, \n",
    "                       verbose_eval=100, \n",
    "                       eval_train_metric =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test_enc.drop([\"ID\"],axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classfier with tuned hyperparamters\n",
    "lgb_clf = lgb.LGBMClassifier(n_estimators=200, boosting_type = \"gbdt\",\n",
    "    learning_rate = 0.01,\n",
    "    max_depth = 4,\n",
    "    num_leaves = 40, \n",
    "    min_sum_hessian_in_leaf = 0.04,\n",
    "    min_data_in_leaf = 61,\n",
    "    objective = 'binary',\n",
    "    lambda_l1 = 0,\n",
    "    lambda_l2 = 35,\n",
    "    is_unbalance = True,\n",
    "    class_weight = \"balanced\",\n",
    "    feature_fraction = 0.6,\n",
    "    cat_smooth = 20,\n",
    "    bagging_fraction = 0.7,\n",
    "    bagging_freq = 8,\n",
    "    metrics ='auc')\n",
    "\n",
    "## model fitting\n",
    "lgb_clf.fit(X2_enc, y)\n",
    "\n",
    "## submission\n",
    "submission(lgb_clf,X2_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "acv_results = cross_validate(svclassifier, X2_enc, y, cv = 10,return_train_score =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([22649.269274  , 10399.8090055 , 21673.22888589,  6811.99339414,\n",
       "        39778.07141256,  7923.59264469, 12070.7809546 , 11497.37369609,\n",
       "        15605.28890753, 48036.19392228]),\n",
       " 'score_time': array([2.89868355, 3.73362684, 2.06549835, 2.35109138, 2.04566884,\n",
       "        1.91014409, 1.90039968, 1.97342229, 1.92709255, 2.09032583]),\n",
       " 'test_score': array([0.81842052, 0.81842052, 0.81842052, 0.81842052, 0.81858586,\n",
       "        0.81858586, 0.81838384, 0.81838384, 0.81838384, 0.81838384]),\n",
       " 'train_score': array([0.81844096, 0.81844096, 0.81844096, 0.81844096, 0.81842259,\n",
       "        0.81842259, 0.81844503, 0.81844503, 0.81844503, 0.81844503])}"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:41:34] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { num_boost_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('LR',\n",
       "                              Pipeline(steps=[('smote', SMOTE(random_state=42)),\n",
       "                                              ('polynomialfeatures',\n",
       "                                               PolynomialFeatures(interaction_only=True)),\n",
       "                                              ('logisticregression',\n",
       "                                               LogisticRegression())])),\n",
       "                             ('RF',\n",
       "                              Pipeline(steps=[('smote', SMOTE(random_state=42)),\n",
       "                                              ('randomforestclassifier',\n",
       "                                               RandomForestClassifier(random_state=13))])),\n",
       "                             ('XGB',\n",
       "                              XGBClassifier(base_sco...\n",
       "                                            learning_rate=None,\n",
       "                                            max_delta_step=None, max_depth=11,\n",
       "                                            min_child_weight=6, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_boost_round=300,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective='reg:logistic',\n",
       "                                            random_state=None, reg_alpha=None,\n",
       "                                            reg_lambda=None, scale_pos_weight=3,\n",
       "                                            subsample=0.6, tree_method=None,\n",
       "                                            validate_parameters=None,\n",
       "                                            verbosity=None))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group / ensemble of models \n",
    "estimator = [] \n",
    "\n",
    "## Logistic regression\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "log_imba_pipeline = make_pipeline(SMOTE(random_state=42),PolynomialFeatures(interaction_only=True),LogisticRegression())\n",
    "\n",
    "## Random forest\n",
    "df_imba_pipeline = make_pipeline(SMOTE(random_state=42),RandomForestClassifier(random_state=13))\n",
    "\n",
    "## XGBoost\n",
    "xgb = XGBClassifier(subsample = 0.6,\n",
    "              scale_pos_weight = 3,\n",
    "              objective = \"reg:logistic\",\n",
    "              min_child_weight = 6,\n",
    "              max_depth = 11,\n",
    "              gamma = 0,\n",
    "              eta = 0.2,\n",
    "              colsample_bytree = 0.7,\n",
    "              booster = 'gbtree',\n",
    "              num_boost_round=300)\n",
    "\n",
    "estimator.append(('LR',log_imba_pipeline)) \n",
    "estimator.append(('RF', df_imba_pipeline)) \n",
    "estimator.append(('XGB', xgb)) \n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft') \n",
    "vot_soft.fit(X2_enc, y) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
