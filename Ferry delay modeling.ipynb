{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib import dates\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.ensemble import VotingClassifier \n",
    "import warnings\n",
    "import holidays\n",
    "import calendar\n",
    "from varname import nameof\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function of getting the optimized paramters and score\n",
    "def hypertuning_rscv(est,p_distr,nbr_iter,X,y):\n",
    "    rdmsearch = RandomizedSearchCV(est, param_distributions=p_distr,n_jobs=-1, n_iter=nbr_iter,cv=5)\n",
    "    \n",
    "    rdmsearch.fit(X,y)\n",
    "    ht_params = rdmsearch.best_params_\n",
    "    ht_score = rdmsearch.best_score_\n",
    "    return(ht_params,ht_score)\n",
    "\n",
    "## submission\n",
    "def submission(clf,test):\n",
    "    preds = clf.predict_proba(test)\n",
    "    test[\"ID\"] = test.index +1\n",
    "    ctbsubmission = pd.concat([pd.DataFrame(preds[:,1]),test['ID']],axis=1)\n",
    "    ctbsubmission.columns = ['Delay.Indicator','ID']\n",
    "    ctbsubmission.to_csv('{}_predictions_1.csv'.format(nameof(clf)),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOptimizer:\n",
    "    best_score = None\n",
    "    opt = None\n",
    "    \n",
    "    def __init__(self, model, X_train, y_train, categorical_columns_indices=None, n_fold=3, seed=2405, early_stopping_rounds=30, is_stratified=True, is_shuffle=True):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.categorical_columns_indices = categorical_columns_indices\n",
    "        self.n_fold = n_fold\n",
    "        self.seed = seed\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.is_stratified = is_stratified\n",
    "        self.is_shuffle = is_shuffle\n",
    "        \n",
    "        \n",
    "    def update_model(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self.model, k, v)\n",
    "            \n",
    "    def evaluate_model(self):\n",
    "        pass\n",
    "    \n",
    "    def optimize(self, param_space, max_evals=10, n_random_starts=2):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        @use_named_args(param_space)\n",
    "        def _minimize(**params):\n",
    "            self.model.set_params(**params)\n",
    "            return self.evaluate_model()\n",
    "        \n",
    "        opt = gp_minimize(_minimize, param_space, n_calls=max_evals, n_random_starts=n_random_starts, random_state=2405, n_jobs=-1)\n",
    "        best_values = opt.x\n",
    "        optimal_values = dict(zip([param.name for param in param_space], best_values))\n",
    "        best_score = opt.fun\n",
    "        self.best_score = best_score\n",
    "        self.opt = opt\n",
    "        \n",
    "        print('optimal_parameters: {}\\noptimal score: {}\\noptimization time: {}'.format(optimal_values, best_score, time.time() - start_time))\n",
    "        print('updating model with optimal values')\n",
    "        self.update_model(**optimal_values)\n",
    "        plot_convergence(opt)\n",
    "        return optimal_values\n",
    "\n",
    "class XgbOptimizer(ModelOptimizer):\n",
    "    def evaluate_model(self):\n",
    "        scores = xgboost.cv(self.model.get_xgb_params(), \n",
    "                    xgboost.DMatrix(self.X_train, label=self.y_train),\n",
    "                    num_boost_round=self.model.n_estimators, \n",
    "                    metrics='auc', \n",
    "                    nfold=self.n_fold, \n",
    "                    stratified=self.is_stratified,\n",
    "                    shuffle=self.is_shuffle,\n",
    "                    seed=self.seed,\n",
    "                    early_stopping_rounds=self.early_stopping_rounds)\n",
    "        self.scores = scores\n",
    "        test_scores = scores.iloc[:, 2]\n",
    "        best_metric = test_scores.max()\n",
    "        return 1 - best_metric\n",
    "    \n",
    "class CatboostOptimizer(ModelOptimizer):\n",
    "    def evaluate_model(self):\n",
    "        validation_scores = catboost.cv(\n",
    "        catboost.Pool(self.X_train, \n",
    "                      self.y_train, \n",
    "                      cat_features=self.categorical_columns_indices),\n",
    "        self.model.get_params(), \n",
    "        nfold=self.n_fold,\n",
    "        stratified=self.is_stratified,\n",
    "        seed=self.seed,\n",
    "        early_stopping_rounds=self.early_stopping_rounds,\n",
    "        shuffle=self.is_shuffle,\n",
    "        plot=False)\n",
    "        self.scores = validation_scores\n",
    "        test_scores = validation_scores.iloc[:, 2]\n",
    "        best_metric = test_scores.max()\n",
    "        return 1 - best_metric\n",
    "    \n",
    "class LightGBMOptimizer(ModelOptimizer):\n",
    "    def evaluate_model(self):\n",
    "        lgb_dataset = lgb.Dataset(self.X_train, \n",
    "                                  self.y_train, \n",
    "                                  self.categorical_columns_indices)\n",
    "        eval_hist = lgb.cv(self.model.get_params(), \n",
    "                           lgb_dataset,\n",
    "                           self.model.n_estimators, \n",
    "                           nfold=self.n_fold,\n",
    "                           seed=self.seed, \n",
    "                           stratified=self.is_stratified, \n",
    "                           shuffle=self.is_shuffle,\n",
    "                           early_stopping_rounds=self.early_stopping_rounds, \n",
    "                           metrics='auc')\n",
    "        self.scores = eval_hist\n",
    "        test_scores = eval_hist[list(eval_hist.keys())[0]]\n",
    "        best_metric = max(test_scores)\n",
    "        return 1 - best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"traffic_clean3.csv\")\n",
    "train1 = pd.read_csv(\"train3.csv\")\n",
    "test = pd.read_csv(\"test_clean3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1[[\"lag1\",\"lag2\",\"lag3\",\"lag4\",\"lag_std1\",\"lag_std2\",\"lag_std3\"]] = train[[\"lag1\",\"lag2\",\"lag3\",\"lag4\",\"lag_std1\",\"lag_std2\",\"lag_std3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary feature preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the Vessel.name categories that are not present in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have noticed that there are some Vessel.Name categorical levels in the training dataset that are not present in the test data. Considering that the redundant information would negatively influence the predictive power of our models, we decided to cut out the Vessel name categorical levels that are not seen in the test data from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_redun = [i for i in np.unique(train1[\"Vessel.Name\"]) if i not in np.unique(test[\"Vessel.Name\"])]\n",
    "train1 = train1.loc[~train1[\"Vessel.Name\"].isin(vessel_redun),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize the traffic lags from 0 to 5 to reduce noises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distribution of the lag variables are significant imbalanced, we decided to categorize traffic lags into categroical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trai = train1[\"Scheduled.Departure\"].apply(lambda x: pd.to_datetime(x).strftime(\"%H:%M\"))\n",
    "tes = test[\"Scheduled.Departure\"].apply(lambda x: pd.to_datetime(x).strftime(\"%H:%M\"))\n",
    "vessel_sche = [i for i in np.unique(trai) if i not in np.unique(tes)]\n",
    "train1 = train1.loc[~train1[\"Scheduled.Departure\"].isin(vessel_sche),:]\n",
    "\n",
    "train1.reset_index(drop = True,inplace = True)\n",
    "\n",
    "## train cut\n",
    "train1[\"cut1\"] = pd.cut(train.lag1 , [0,2,3,4,5], right=True)\n",
    "train1[\"cut2\"] = pd.cut(train.lag2 , [0,2,3,4,5], right=True)\n",
    "train1[\"cut3\"] = pd.cut(train.lag3 , [0,2,3,4,5], right=True)\n",
    "train1[\"cut4\"] = pd.cut(train.lag4 , [0,2,3,4,5], right=True)\n",
    "\n",
    "## test cut\n",
    "test[\"cut1\"] = pd.cut(test.lag1 , [0,2,3,4,5], right=True)\n",
    "test[\"cut2\"] = pd.cut(test.lag2 , [0,2,3,4,5], right=True)\n",
    "test[\"cut3\"] = pd.cut(test.lag3 , [0,2,3,4,5], right=True)\n",
    "test[\"cut4\"] = pd.cut(test.lag4 , [0,2,3,4,5], right=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Vessel categories with similar delay frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the delay frequency by category\n",
    "train1[[\"Vessel.Name\",\"Delay.Indicator\"]].groupby(\"Vessel.Name\").mean().sort_values(by = 'Delay.Indicator')\n",
    "\n",
    "## combine the Vessel categories with similar delay frequencies\n",
    "train1.loc[train1[\"Vessel.Name\"].isin([\"Skeena Queen\",\"Mayne Queen\",'Queen of Alberni']),\"Vessel\"] = \"name1\"\n",
    "train1.loc[train1[\"Vessel.Name\"].isin([\"Coastal Renaissance\",\"Queen of New Westminster\",\"Bowen Queen\",\"Coastal Inspiration\",\"Queen of Cumberland\",\"Coastal Celebration\"]),\"Vessel\"] = \"name2\"\n",
    "train1.loc[train1[\"Vessel.Name\"].isin(['Queen of Coquitlam',\"Spirit of Vancouver Island\",\"Queen of Cowichan\",\"Queen of Capilano\"]),\"Vessel\"] = \"name3\"\n",
    "train1.loc[train1[\"Vessel.Name\"].isin([\"Queen of Oak Bay\",\"Salish Raven\",\"Queen of Surrey\",\"Salish Eagle\"]),\"Vessel\"] = \"name4\"\n",
    "\n",
    "test.loc[train1[\"Vessel.Name\"].isin([\"Skeena Queen\",\"Mayne Queen\",'Queen of Alberni']),\"Vessel\"] = \"name1\"\n",
    "test.loc[train1[\"Vessel.Name\"].isin([\"Coastal Renaissance\",\"Queen of New Westminster\",\"Bowen Queen\",\"Coastal Inspiration\",\"Queen of Cumberland\",\"Coastal Celebration\"]),\"Vessel\"] = \"name2\"\n",
    "test.loc[train1[\"Vessel.Name\"].isin(['Queen of Coquitlam',\"Spirit of Vancouver Island\",\"Queen of Cowichan\",\"Queen of Capilano\"]),\"Vessel\"] = \"name3\"\n",
    "test.loc[train1[\"Vessel.Name\"].isin([\"Queen of Oak Bay\",\"Salish Raven\",\"Queen of Surrey\",\"Salish Eagle\"]),\"Vessel\"] = \"name4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the feature Round Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the feature Round Hour\n",
    "train1.drop(\"Unnamed: 0\",axis=1,inplace =True)\n",
    "train1[\"Date_time\"] = pd.to_datetime(train1[\"Date_time\"])\n",
    "test[\"Date_time\"] = pd.to_datetime(test[\"Date_time\"])\n",
    "\n",
    "train1[\"Round_time\"] = train1[\"Date_time\"].dt.round(\"H\")\n",
    "train1[\"Round_Hour\"] = train1[\"Round_time\"].apply(lambda x: x.strftime(\"%H\"))\n",
    "\n",
    "test[\"Round_time\"] = test[\"Date_time\"].dt.round(\"H\")\n",
    "test[\"Round_Hour\"] = test[\"Round_time\"].apply(lambda x: x.strftime(\"%H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test.Round_Hour == \"00\",\"Round_Hour\"] = \"23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1[\"Month\"] = train1[\"Month\"].astype(\"int\")\n",
    "test[\"Month\"] = test[\"Month\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train1.loc[train1[\"Status\"].isin([\"On Time\",\"Traffic delay\",\"Operational delay\",\"Mechanical issue\"]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1[\"Month\"] = train1[\"Month\"].astype(\"int\")\n",
    "train1[\"Day\"] = train1[\"Day\"].astype(\"object\")\n",
    "\n",
    "test[\"Month\"] = test[\"Month\"].astype(\"int\")\n",
    "test[\"Day\"] = test[\"Day\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = train1[[\"Round_Hour\",\"Day\",\"Month\",\"Day.of.Month\",\"Num.of.sailings\",\n",
    "       'Vessel.Name_Kfold_Target_Enc','Trip_Kfold_Target_Enc',\"holidays_indicator\",\"lag2\",\"lag3\",\"lag4\",\"lag_std1\",\"lag_std2\",\"weekend.Indicator\"]]\n",
    "X2_enc = pd.get_dummies(X2,prefix=[\"Day\",\"Round_Hour\"])\n",
    "\n",
    "y = train1[\"Delay.Indicator\"]\n",
    "\n",
    "X_test2 = test[[\"Round_Hour\",\"Day\",\"Month\",\"Day.of.Month\",\"Num.of.sailings\",\n",
    "       'Vessel.Name_Kfold_Target_Enc','Trip_Kfold_Target_Enc',\"holidays_indicator\",\"lag2\",\"lag3\",\"lag4\",\"lag_std1\",\"lag_std2\",\"weekend.Indicator\"]]\n",
    "\n",
    "X2_test_enc = pd.get_dummies(X_test2,prefix=[\"Day\",\"Round_Hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test_enc.drop(\"ID\",axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "imba_pipeline = make_pipeline(SMOTE(random_state=42),\n",
    "                     PolynomialFeatures(interaction_only=True), \n",
    "                     logreg)\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [1]}\n",
    "\n",
    "nbr_iter=200\n",
    "random_params, random_score = hypertuning_rscv(imba_pipeline, param_grid,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "imba_pipeline_lg = make_pipeline(SMOTE(random_state=42),PolynomialFeatures(interaction_only=True), clf)\n",
    "\n",
    "cv_results_auc_log = cross_val_score(imba_pipeline_lg, X2_enc, y, scoring='roc_auc', cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7350214765097542"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_auc_log.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('smote', SMOTE(random_state=42)),\n",
       "                ('polynomialfeatures',\n",
       "                 PolynomialFeatures(interaction_only=True)),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imba_pipeline_lg.fit(X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logistic regression prediction submitted\n",
    "submission(imba_pipeline_lg,X2_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For random forest implementation, we use random search to tune hyperparameters to find the maximal 3 fold cross validation\n",
    "- Oversampling (SMOTE)\n",
    "- Undersampling \n",
    "- class_weight (a hyperparameter accomodating imbalanced dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling by SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split':[2,3,5,6,8,10,12],\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [i for i in range(1,6)],\n",
    "    'max_features': [i for i in range(6,12)],\n",
    "    'min_samples_leaf': [3,4,5,6,7],\n",
    "    'min_samples_split': [2,3,5,6],\n",
    "    'n_estimators': [300,500,800]\n",
    "    #\"class_weight\":[\"balanced\",\"balanced_subsample\"]\n",
    "}\n",
    "nbr_iter = 200\n",
    "new_params = {'randomforestclassifier__' + key: param_grid[key] for key in param_grid}\n",
    "imba_pipeline = make_pipeline(SMOTE(random_state=42),RandomForestClassifier(random_state=13))\n",
    "\n",
    "\n",
    "random_params, random_score = hypertuning_rscv(imba_pipeline, new_params,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 300,\n",
       " 'randomforestclassifier__min_samples_split': 3,\n",
       " 'randomforestclassifier__min_samples_leaf': 4,\n",
       " 'randomforestclassifier__max_features': 6,\n",
       " 'randomforestclassifier__max_depth': 2,\n",
       " 'randomforestclassifier__criterion': 'gini',\n",
       " 'randomforestclassifier__bootstrap': True}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300,\n",
    " min_samples_split = 3,\n",
    " min_samples_leaf = 4,\n",
    " max_features = 6,\n",
    " max_depth = 2,\n",
    " criterion = 'gini',\n",
    " #class_weight = \"balanced_subsample\",\n",
    " bootstrap = True,\n",
    " n_jobs=-1)\n",
    "\n",
    "imba_pipeline_rf_SMOTE = make_pipeline(SMOTE(random_state=42),clf)\n",
    "\n",
    "crossval_scores = cross_validate(imba_pipeline_rf_SMOTE, X2_enc, y, cv = 10,scoring = \"roc_auc\",return_train_score =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70422465, 0.75364616, 0.63425216, 0.52105357, 0.69579012,\n",
       "       0.68146531, 0.67068015, 0.66615306, 0.68821541, 0.70747874])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_scores[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest prediction submmited (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(imba_pipeline_rf_SMOTE,,X2_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resampling\n",
    "resample = TomekLinks()\n",
    "\n",
    "## undersampling pipeline\n",
    "imba_pipeline_rf_un = make_pipeline(resample,RandomForestClassifier(random_state=13))\n",
    "\n",
    "## tuned hyperparameter\n",
    "random_params, random_score = hypertuning_rscv(imba_pipeline_rf_un, new_params,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 800,\n",
       " 'randomforestclassifier__min_samples_split': 5,\n",
       " 'randomforestclassifier__min_samples_leaf': 4,\n",
       " 'randomforestclassifier__max_features': 7,\n",
       " 'randomforestclassifier__max_depth': 2,\n",
       " 'randomforestclassifier__criterion': 'gini',\n",
       " 'randomforestclassifier__bootstrap': True}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 800,\n",
    " min_samples_split = 5,\n",
    " min_samples_leaf = 4,\n",
    " max_features = 7,\n",
    " max_depth = 2,\n",
    " criterion = 'gini',\n",
    " bootstrap = True,\n",
    " n_jobs=-1)\n",
    "\n",
    "imba_pipeline_rf = make_pipeline(resample,clf)\n",
    "\n",
    "## cross validation\n",
    "crossval_scores = cross_validate(imba_pipeline_rf, X2_enc, y, cv = 10,scoring = \"roc_auc\",return_train_score =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split':[2,3,5,6,8,10,12],\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [i for i in range(1,6)],\n",
    "    'max_features': [i for i in range(6,12)],\n",
    "    'min_samples_leaf': [3,4,5,6,7],\n",
    "    'min_samples_split': [2,3,5,6],\n",
    "    'n_estimators': [300,500,800],\n",
    "    \"class_weight\":[\"balanced\",\"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "clf_3 = RandomForestClassifier()\n",
    "\n",
    "random_params, random_score = hypertuning_rscv(clf_3, param_grid,nbr_iter,X2_enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800,\n",
       " 'min_samples_split': 6,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 7,\n",
       " 'max_depth': 1,\n",
       " 'criterion': 'entropy',\n",
       " 'class_weight': 'balanced',\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300,\n",
    " min_samples_split = 3,\n",
    " min_samples_leaf = 4,\n",
    " max_features = 10,\n",
    " max_depth = 1,\n",
    " criterion = 'entropy',\n",
    " class_weight = 'balanced_subsample',\n",
    " bootstrap = True,\n",
    " n_jobs=-1)\n",
    "\n",
    "## cross validation\n",
    "crossval_scores = cross_validate(clf, X2_enc, y, cv = 10,scoring = \"roc_auc\",return_train_score =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6749240107454988"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X2_enc,y)\n",
    "\n",
    "submission(clf,X2_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2[\"Round_Hour\"] = X2[\"Round_Hour\"].astype(\"category\")\n",
    "X2[\"Day\"] = X2[\"Day\"].astype(\"category\")\n",
    "#X2[\"Day.of.Month\"] = X2[\"Day.of.Month\"].astype(\"category\")\n",
    "X2[\"Month\"] = X2[\"Month\"].astype(\"category\")\n",
    "\n",
    "\n",
    "X_test2[\"Round_Hour\"] = X_test2[\"Round_Hour\"].astype(\"category\")\n",
    "X_test2[\"Day\"] = X_test2[\"Day\"].astype(\"category\")\n",
    "#X_test2[\"Day.of.Month\"] = X_test2[\"Day.of.Month\"].astype(\"category\")\n",
    "X_test2[\"Month\"] = X_test2[\"Month\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2[\"Month\"] = X2[\"Month\"].astype(\"category\")\n",
    "#X2[\"Day.of.Month\"] = X2[\"Day.of.Month\"].astype(\"category\")\n",
    "X2_encode = pd.get_dummies(X2,prefix=[\"Round_Hour\",\"Day\",\"Month\"])\n",
    "\n",
    "#X_test2[\"Day.of.Month\"] = X_test2[\"Day.of.Month\"].astype(\"category\")\n",
    "X_test2[\"Month\"] = X_test2[\"Month\"].astype(\"category\")\n",
    "X_test_encode = pd.get_dummies(X_test2,prefix=[\"Round_Hour\",\"Day\",\"Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_encode.rename(columns = {\"cut4_(0, 2]\":\"cut1\",\"cut4_(2, 3]\":\"cut2\",\"cut4_(3, 4]\":\"cut3\",\"cut4_(4, 5]\":\"cut4\"},inplace =True)\n",
    "X_test_encode.rename(columns = {\"cut4_(0, 2]\":\"cut1\",\"cut4_(2, 3]\":\"cut2\",\"cut4_(3, 4]\":\"cut3\",\"cut4_(4, 5]\":\"cut4\"},inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X2_encode, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "p_distr ={\n",
    "        \"objective\":['reg:logistic'],\n",
    "        \"booster\":[\"gbtree\"],\n",
    "        'n_estimators': [500,700],\n",
    "        'min_child_weight': list(range(1,10,1)),\n",
    "        'gamma': [0,1,3,5,7,10],\n",
    "        'subsample': [0.6,0.7,0.8],\n",
    "        'colsample_bytree': [0.7],\n",
    "        'max_depth': list(range(8,12,1)),\n",
    "        'eta': [0.1,0.2],  \n",
    "        \"scale_pos_weight\":[3,5,6],\n",
    "        #\"lambda\":[0.4],\n",
    "}\n",
    "model = XGBClassifier()\n",
    "nbr_iter = 168\n",
    "random_params, random_score = hypertuning_rscv(model, p_distr, nbr_iter,X2_encode,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.6,\n",
       " 'scale_pos_weight': 3,\n",
       " 'objective': 'reg:logistic',\n",
       " 'n_estimators': 500,\n",
       " 'min_child_weight': 1,\n",
       " 'max_depth': 8,\n",
       " 'gamma': 0,\n",
       " 'eta': 0.1,\n",
       " 'colsample_bytree': 0.7,\n",
       " 'booster': 'gbtree'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:14:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " cv_results = xgb.cv(dtrain= dtrain, params=random_params, nfold=10,num_boost_round=300,\n",
    "                     metrics='auc', early_stopping_rounds = 50,\n",
    "                        as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768170</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.738616</td>\n",
       "      <td>0.010368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.801723</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.771416</td>\n",
       "      <td>0.013302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.815890</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>0.787373</td>\n",
       "      <td>0.008863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.823562</td>\n",
       "      <td>0.004091</td>\n",
       "      <td>0.794752</td>\n",
       "      <td>0.008909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.829023</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.800170</td>\n",
       "      <td>0.009660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.982038</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.854014</td>\n",
       "      <td>0.008391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.982139</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.854025</td>\n",
       "      <td>0.008352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.982253</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.853958</td>\n",
       "      <td>0.008334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.982373</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.853991</td>\n",
       "      <td>0.008384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.982521</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.854077</td>\n",
       "      <td>0.008418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0          0.768170       0.002473       0.738616      0.010368\n",
       "1          0.801723       0.008453       0.771416      0.013302\n",
       "2          0.815890       0.005237       0.787373      0.008863\n",
       "3          0.823562       0.004091       0.794752      0.008909\n",
       "4          0.829023       0.002847       0.800170      0.009660\n",
       "..              ...            ...            ...           ...\n",
       "295        0.982038       0.000477       0.854014      0.008391\n",
       "296        0.982139       0.000484       0.854025      0.008352\n",
       "297        0.982253       0.000510       0.853958      0.008334\n",
       "298        0.982373       0.000529       0.853991      0.008384\n",
       "299        0.982521       0.000538       0.854077      0.008418\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:17] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { num_boost_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBClassifier(subsample = 0.6,\n",
    "              scale_pos_weight = 3,\n",
    "              objective = \"reg:logistic\",\n",
    "              min_child_weight = 1,\n",
    "              n_estimators = 500,\n",
    "              max_depth = 8,\n",
    "              gamma = 0,\n",
    "              eta = 0.1,\n",
    "              colsample_bytree = 0.7,\n",
    "              booster = 'gbtree',\n",
    "              num_boost_round=300)\n",
    "\n",
    "xg_reg.fit(X2_enc,y)\n",
    "## submission\n",
    "submission(xg_reg,X2_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM hyperparameter tuning<a class=\"tocSkip\">\n",
    "Since hyperparameter tuning is a time consuming process, we will take 7 steps to perform hyperparamter tuning by AUC: \n",
    "- step 1: tuning **max_depth** and **num_leaves** (tree complexity)\n",
    "- step 2: tuning **min_data_in_leaf** and **min_sum_hessian_in_leaf** (prevent overfitting)\n",
    "- step 3: tuning **feature_fraction** (prevent overfitting, decorrelate trees)\n",
    "- step 4: tuning **bagging_fraction** and **bagging_freq** (prevent overfitting)\n",
    "- step 5: tuning **lambda_l1(reg_alpha)** and **lambda_l2(reg_lambda)** (prevent overfitting)\n",
    "- step 6: tuning **cat_smooth** (reduce the effect of noises in categorical features)\n",
    "- step 7: tuning **learning_rate** and **num_iterations** (final tuning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X2_encode,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning all lightGBM hyperparameters in one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_lgb = {\n",
    "    'max_depth': [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],\n",
    "    'num_leaves': [40,50,60,70,80,90,100],\n",
    "    'min_data_in_leaf':range(1,102,10),\n",
    "    'min_sum_hessian_in_leaf':[0.02,0.03,0.04,0.05],\n",
    "    'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "    'bagging_freq': [2, 4, 5, 6, 8],\n",
    "    'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n",
    "    'lambda_l2': [0, 10, 15, 35, 40],\n",
    "    'cat_smooth': [1, 10, 15, 20, 35],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "    #'num_iterations':range(100,1000,100)\n",
    "}\n",
    "\n",
    "\n",
    "model_lgb = lgb.LGBMClassifier(is_unbalance = True,metric = 'auc')\n",
    "\n",
    "random_params_lgb, random_score_lgb = hypertuning_rscv(model_lgb, para_lgb,nbr_iter,X2_encode,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 40,\n",
       " 'min_sum_hessian_in_leaf': 0.04,\n",
       " 'min_data_in_leaf': 61,\n",
       " 'max_depth': 4,\n",
       " 'learning_rate': 0.01,\n",
       " 'lambda_l2': 35,\n",
       " 'lambda_l1': 0,\n",
       " 'feature_fraction': 0.6,\n",
       " 'cat_smooth': 20,\n",
       " 'bagging_freq': 8,\n",
       " 'bagging_fraction': 0.7}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_params_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning by min_data_in_leaf and min_sum_hessian_in_leaf  (step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's train auc: 0.775119 + 0.00111859\tcv_agg's valid auc: 0.769922 + 0.0104099\n",
      "[200]\tcv_agg's train auc: 0.787696 + 0.00127297\tcv_agg's valid auc: 0.781594 + 0.00964075\n",
      "[300]\tcv_agg's train auc: 0.796683 + 0.00114024\tcv_agg's valid auc: 0.789772 + 0.0100093\n",
      "[400]\tcv_agg's train auc: 0.803041 + 0.0012455\tcv_agg's valid auc: 0.795453 + 0.0100901\n",
      "[500]\tcv_agg's train auc: 0.80791 + 0.00112774\tcv_agg's valid auc: 0.799375 + 0.0101369\n",
      "[600]\tcv_agg's train auc: 0.811825 + 0.0010572\tcv_agg's valid auc: 0.802474 + 0.0103189\n",
      "[700]\tcv_agg's train auc: 0.815412 + 0.00111558\tcv_agg's valid auc: 0.805255 + 0.0102686\n",
      "[800]\tcv_agg's train auc: 0.81867 + 0.0010543\tcv_agg's valid auc: 0.807684 + 0.0104024\n",
      "[900]\tcv_agg's train auc: 0.822027 + 0.00098294\tcv_agg's valid auc: 0.810171 + 0.0104422\n",
      "[1000]\tcv_agg's train auc: 0.824941 + 0.00110989\tcv_agg's valid auc: 0.812159 + 0.0105018\n"
     ]
    }
   ],
   "source": [
    "# LightGBM, cross-validation\n",
    "cv_result_lgb = lgb.cv(random_params_lgb, \n",
    "                       dtrain, \n",
    "                       num_boost_round = 1000, \n",
    "                       metrics = \"auc\",\n",
    "                       nfold=10, \n",
    "                       stratified=True, \n",
    "                       early_stopping_rounds=50, \n",
    "                       verbose_eval=100, \n",
    "                       eval_train_metric =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test_enc.drop([\"ID\"],axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classfier with tuned hyperparamters\n",
    "lgb_clf = lgb.LGBMClassifier(n_estimators=200, boosting_type = \"gbdt\",\n",
    "    learning_rate = 0.01,\n",
    "    max_depth = 4,\n",
    "    num_leaves = 40, \n",
    "    min_sum_hessian_in_leaf = 0.04,\n",
    "    min_data_in_leaf = 61,\n",
    "    objective = 'binary',\n",
    "    lambda_l1 = 0,\n",
    "    lambda_l2 = 35,\n",
    "    is_unbalance = True,\n",
    "    class_weight = \"balanced\",\n",
    "    feature_fraction = 0.6,\n",
    "    cat_smooth = 20,\n",
    "    bagging_fraction = 0.7,\n",
    "    bagging_freq = 8,\n",
    "    metrics ='auc')\n",
    "\n",
    "## model fitting\n",
    "lgb_clf.fit(X2_enc, y)\n",
    "\n",
    "## submission\n",
    "submission(lgb_clf,X2_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "acv_results = cross_validate(svclassifier, X2_enc, y, cv = 10,return_train_score =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([22649.269274  , 10399.8090055 , 21673.22888589,  6811.99339414,\n",
       "        39778.07141256,  7923.59264469, 12070.7809546 , 11497.37369609,\n",
       "        15605.28890753, 48036.19392228]),\n",
       " 'score_time': array([2.89868355, 3.73362684, 2.06549835, 2.35109138, 2.04566884,\n",
       "        1.91014409, 1.90039968, 1.97342229, 1.92709255, 2.09032583]),\n",
       " 'test_score': array([0.81842052, 0.81842052, 0.81842052, 0.81842052, 0.81858586,\n",
       "        0.81858586, 0.81838384, 0.81838384, 0.81838384, 0.81838384]),\n",
       " 'train_score': array([0.81844096, 0.81844096, 0.81844096, 0.81844096, 0.81842259,\n",
       "        0.81842259, 0.81844503, 0.81844503, 0.81844503, 0.81844503])}"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:41:34] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { num_boost_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('LR',\n",
       "                              Pipeline(steps=[('smote', SMOTE(random_state=42)),\n",
       "                                              ('polynomialfeatures',\n",
       "                                               PolynomialFeatures(interaction_only=True)),\n",
       "                                              ('logisticregression',\n",
       "                                               LogisticRegression())])),\n",
       "                             ('RF',\n",
       "                              Pipeline(steps=[('smote', SMOTE(random_state=42)),\n",
       "                                              ('randomforestclassifier',\n",
       "                                               RandomForestClassifier(random_state=13))])),\n",
       "                             ('XGB',\n",
       "                              XGBClassifier(base_sco...\n",
       "                                            learning_rate=None,\n",
       "                                            max_delta_step=None, max_depth=11,\n",
       "                                            min_child_weight=6, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_boost_round=300,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective='reg:logistic',\n",
       "                                            random_state=None, reg_alpha=None,\n",
       "                                            reg_lambda=None, scale_pos_weight=3,\n",
       "                                            subsample=0.6, tree_method=None,\n",
       "                                            validate_parameters=None,\n",
       "                                            verbosity=None))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group / ensemble of models \n",
    "estimator = [] \n",
    "\n",
    "## Logistic regression\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "log_imba_pipeline = make_pipeline(SMOTE(random_state=42),PolynomialFeatures(interaction_only=True),LogisticRegression())\n",
    "\n",
    "## Random forest\n",
    "df_imba_pipeline = make_pipeline(SMOTE(random_state=42),RandomForestClassifier(random_state=13))\n",
    "\n",
    "## XGBoost\n",
    "xgb = XGBClassifier(subsample = 0.6,\n",
    "              scale_pos_weight = 3,\n",
    "              objective = \"reg:logistic\",\n",
    "              min_child_weight = 6,\n",
    "              max_depth = 11,\n",
    "              gamma = 0,\n",
    "              eta = 0.2,\n",
    "              colsample_bytree = 0.7,\n",
    "              booster = 'gbtree',\n",
    "              num_boost_round=300)\n",
    "\n",
    "estimator.append(('LR',log_imba_pipeline)) \n",
    "estimator.append(('RF', df_imba_pipeline)) \n",
    "estimator.append(('XGB', xgb)) \n",
    "\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft') \n",
    "vot_soft.fit(X2_enc, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
